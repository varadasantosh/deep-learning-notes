{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS2MRB5biP0Cy80btq7zar",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varadasantosh/deep-learning-notes/blob/tensorflow/Rotary_Embeddings_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QDQ4m-0csQ3L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def determine_rotation_theta(max_seqlen, d_model):\n",
        "\n",
        "  \"\"\" This method takes Sequence Length , Dimensions of Embeddings to calculate the angle for\n",
        "      each position in the sequence\n",
        "  \"\"\"\n",
        "  theta = 1/torch.pow(10000,torch.arange(0,d_model,2)/d_model)\n",
        "  positions = torch.arange(0,max_seqlen)\n",
        "  position_theta = positions.unsqueeze(1) * theta.unsqueeze(0)\n",
        "  position_theta = torch.stack((position_theta.cos(),position_theta.sin()),dim=2).flatten(1)\n",
        "  return  position_theta\n",
        "\n",
        "def calc_rotary_embeddings(embeddings):\n",
        "\n",
        "  batch_size,max_seqlen,d_model= embeddings.shape\n",
        "  rotation_theta = determine_rotation_theta(max_seqlen,d_model)\n",
        "  cos_theta = rotation_theta[...,0::2]\n",
        "  sin_theta = rotation_theta[...,1::2]\n",
        "\n",
        "  embeddings[...,0::2] =  embeddings[...,0::2] * cos_theta  + embeddings[...,1::2] * sin_theta\n",
        "  embeddings[...,1::2] =  embeddings[...,0::2] * sin_theta  + embeddings[...,1::2] * cos_theta\n",
        "  return embeddings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings= torch.randn(1,4,8)\n",
        "rotated_embeddings = calc_rotary_embeddings(embeddings)\n",
        "rotated_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3A91rMw5USy",
        "outputId": "d6a4ecd0-7fb5-4f2f-e479-6f7dd0aaf1fd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# determine_rotation_theta\n",
        "\n",
        "Step 1:- This line of code is to calculate θ for each position , the formaule\n",
        "          for calculating θ = 10000^(2i/d)\n",
        "   \n",
        "    theta = 1/torch.pow(10000,torch.arange(0,d_model,2)/d_model)\n",
        "\n",
        "Step 2:- Construct Positions tensor which corresponds to each token in a\n",
        "          sequence   \n",
        "\n",
        "    positions = torch.arange(0,max_seqlen)\n",
        "\n",
        "Step 3:- for each token and each embedding pair the angle needs to be  \n",
        "         calculated , the final angle depends on the position of the token\n",
        "         and index of an embedding pair , meaning if the sequence is of lenght\n",
        "         **4** and Embeddings are of length **8**, because we rotate the embedings in 2D space, hence from the maths and theory we looked into\n",
        "         we rotate pair of embeddings hence we will have embeddings of pairs\n",
        "         (0,1) (2,3) (4,5) (6,7) for all tokens, below code helps in doing the same , positions.unsqueeze results in dimension (4*1), theta.unsqueeze(0) results in 1*4 hence the final result would be of dimension (4*4)\n",
        "\n",
        "    position_theta = positions.unsqueeze(1) * theta.unsqueeze(0)     \n",
        "\n",
        "Step 4:- We need to calculate the cosθ and sinθ required for all θ for all the\n",
        "         tokens and embedding pairs , this is one of the important part of calcualtion, to understand this bit more let us take an example\n",
        "\n",
        "    position_theta = torch.stack((position_theta.cos(),position_theta.sin()),dim=2).flatten(1)      \n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "HdLWr_OZ5TnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cos= torch.arange(1,9).reshape(2,4)\n",
        "sin= torch.arange(9,17).reshape(2,4)"
      ],
      "metadata": {
        "id": "CmtWSNBw-e8b"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stack Operation\n",
        "\n",
        "  Stack Operation by default does stacking along zeroth dimension, but we would need to calculate cosθ & sinθ for all the angles (each token position and each embedding pair), hence we need to stack this along the last dimenstion, which is **2** , we will see the effect of this"
      ],
      "metadata": {
        "id": "nqIRODZ0-p41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack((cos,sin))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6GRYA7D-mEe",
        "outputId": "c5f4353f-6e1a-451d-a9a8-b1583aba77ca"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1,  2,  3,  4],\n",
              "         [ 5,  6,  7,  8]],\n",
              "\n",
              "        [[ 9, 10, 11, 12],\n",
              "         [13, 14, 15, 16]]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack((cos,sin)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO4rCYU7Did3",
        "outputId": "4711912f-0c3b-48bf-b570-bb07e2c1f006"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking along the last dimension, but this results in different shape from we need, hence we need to flatten this, we can observe the shape before and after  flattening"
      ],
      "metadata": {
        "id": "fGKUo7HL_qQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack((cos,sin),dim=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN0AOHlO_l8-",
        "outputId": "7ece7b87-8ed4-4ce2-de45-3ae96d57c0c6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1,  9],\n",
              "         [ 2, 10],\n",
              "         [ 3, 11],\n",
              "         [ 4, 12]],\n",
              "\n",
              "        [[ 5, 13],\n",
              "         [ 6, 14],\n",
              "         [ 7, 15],\n",
              "         [ 8, 16]]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack((cos,sin),dim=2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRWqBn6YABGO",
        "outputId": "451cccbb-1e40-47b0-cd7e-fa73c51ff3b2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let us Flatten this from dimension **1** , flattening this changes it to below and we can observe the shape , in below , this is required as for each pair we need , though we have 4 pairs, for each pair we need to calculate both cosθ & sinθ, to summarize we started with cos & sin matrices of size(2,4) which corresponds to 2 tokens and each token has embedding dimension of size 8, as 8 results in 4 pairs we have cos & sin of size (2,4) for each embedding pair we need to calculate cos & sin values which results tokens*pairs*2 =2*4*2 =16\n",
        "\n",
        "\n",
        "\n",
        "##PAIR-1\n",
        "\n",
        "|   DIM-1    |   DIM-2    |\n",
        "|------------|------------|\n",
        "| 1 (COSθ)   | 2 (SINθ)   |\n",
        "| 9 (SINθ)   | 10 (SINθ)  |\n",
        "\n",
        "##PAIR-2\n",
        "\n",
        "|   DIM-1    |   DIM-2    |\n",
        "|------------|------------|\n",
        "| 3 (COSθ)   | 4 (SINθ)   |\n",
        "| 11 (SINθ)  | 12 (SINθ)  |\n",
        "\n",
        "##PAIR-3\n",
        "\n",
        "|   DIM-1    |   DIM-2    |\n",
        "|------------|------------|\n",
        "| 5 (COSθ)   | 6 (SINθ)   |\n",
        "| 13 (SINθ)  | 14 (SINθ)  |\n",
        "\n",
        "##PAIR-4\n",
        "\n",
        "|   DIM-1    |   DIM-2    |\n",
        "|------------|------------|\n",
        "| 7 (COSθ)   | 8 (SINθ)   |\n",
        "| 15 (SINθ)  | 16 (SINθ)  |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "esyfwnNb_6tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack((cos,sin),dim=2).flatten(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vixVG1y5_2n3",
        "outputId": "84850064-51f2-445a-b57a-c9bef4a8e5f4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  9,  2, 10,  3, 11,  4, 12],\n",
              "        [ 5, 13,  6, 14,  7, 15,  8, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack((cos,sin),dim=2).flatten(1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AumTYTSgAKmy",
        "outputId": "5a719ef7-113f-4b41-e0a7-3efc5d9abd04"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# calc_rotary_embeddings\n",
        "\n",
        "Step 1:- Find the dimensions of Batch, Sequence Length, Embedding Dimension\n",
        "\n",
        "    batch_size,max_seqlen,d_model= embeddings.shape\n",
        "\n",
        "Step 2:- Determine the rotation angles(θ) required for each position and\n",
        "         embedding pair using the method defined for the same which takes\n",
        "         sequence length & embedding dimensions as input\n",
        "\n",
        "    rotation_theta = determine_rotation_theta(max_seqlen,d_model)\n",
        "\n",
        "Step 3:- Get cosθ and sinθ for each token embedding pair, using below lines of\n",
        "         code , we can see the below to understand the indexes which corresponds to different embedding dimensions\n",
        "\n",
        "    cos_theta = rotation_theta[...,0::2]\n",
        "    sin_theta = rotation_theta[...,1::2]     \n",
        "\n",
        "Step 4:- Finally calculate the Rotation angle for each dimension using below\n",
        "\n",
        "    embeddings[...,0::2] =  embeddings[...,0::2] * cos_theta  + embeddings[...,1::2] * sin_theta\n",
        "    \n",
        "    embeddings[...,1::2] =  embeddings[...,0::2] * sin_theta  + embeddings[...,1::2] * cos_theta\n",
        "\n",
        "   for instance embeddings[0] gives embedding at 0 , embedding at 1 gives embedding at 1 position , now we apply this along with our rotation matrix where x,y corresponds to 0,1\n",
        "\n",
        "   x' = x cosθ - y sinθ \\\n",
        "   y' = x sinθ + y cosθ\n",
        "\n",
        "   R(0) = embedding[0] * cosθ  - embedding[1] * sinθ\n",
        "   R(1) = embedding[0] * sinθ  + embedding[1] * cosθ\n",
        "\n",
        "   $$\n",
        "\\begin{bmatrix} x' \\\\\n",
        "y' \\end{bmatrix} =\n",
        "\\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\\n",
        "\\sin\\theta & \\cos\\theta \\end{bmatrix}\n",
        "\\begin{bmatrix} x \\\\\n",
        "y \\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "x86z277nJtF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotation_theta = torch.stack((cos,sin),dim=2).flatten(1)\n"
      ],
      "metadata": {
        "id": "QZIw2ITCKZRY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotation_theta[...,0::2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36Gvd6bjKvh-",
        "outputId": "45cf6b80-ce49-48f0-8ebd-c772ccbfe5ac"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3, 4],\n",
              "        [5, 6, 7, 8]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rotation_theta[...,1::2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8amzVSzkK95n",
        "outputId": "62506699-ee71-4816-a10d-2108fbba75d5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9, 10, 11, 12],\n",
              "        [13, 14, 15, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ]
}
