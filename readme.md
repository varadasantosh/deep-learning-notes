# Transformers
- [Encoder]
   - [Embeddings]
     - [Positional Embeddings](#Positional-Embeddings)
     - [Rotatory Embeddings](#ROTATORY-EMBEDDINGS)
  - Attention
    - [Self Attention](#SELF-ATTENTION)
    - [Multi Head Attention](#MULTIHEAD-ATTENTION)
    - [Cross Attention](#CROSS-ATTENTION)
    - [Flash Attention](#FLASH-ATTENTION)
  - [Layer Normalization](#LAYER-NORMALIZATION)
  - [FeedForward Layer](#FEED-FORWARD-LAYER)

# Encoder

# Positional-Embeddings

# Rotatory-Embeddings
   https://medium.com/@DataDry/decoding-rotary-positional-embeddings-rope-the-secret-sauce-for-smarter-transformers-193cbc01e4ed

# SELF-ATTENTION
   - https://lilianweng.github.io/posts/2018-06-24-attention/
   - https://github.com/jessevig/bertviz
     
  Evolution of Attention Mechanisms - Attention was firts introduced as part of seq-to-seq (Encoder-Decoder) models in the domain of Neural Machine Translation to translate text from one 
  language to other language. Initial Architectures of encoder-decoder models were composed of encoder and decoder both are RNN's, it is also possible to combine both simple RNN as part of 
  encoder and GRU or LSTM for decoder , encoder takes sentence in source language as input and generates context vector of fixed lentgh , which would be passed as input to Decoder , Decoder 
  takes the context vector and tries to map it to corresponding word or text in target language, this has few limitations as the single context vector generated by the encoder RNN could not 
  capture the entire meaning of the sentence in source language which resulted less accurate results, especially as the length of the sequence grows the accuracy drops.
  
  Inorder to overcome the issues of initial seq-to-seq models, researchers came up with an approach to capture all hidden states of encoder pass them to decoder to capture the meaning or 
  context , but now the challenge is to know which hidden state could be contributing more to find the next word in target language, this is not simple as the source and target languages 
  have different semantics , researchers came up with an approach to build alignment model (Single Layer Feed Forward Neural Network) that takes hidden state of previous decoder timestep $S_{i- 
  1}$ and encoder hidden state $h_{j}$ vector to build context vector $C_{t}$ using alignment model , the alignment model computes the compatability scores between the previous decoder hidden 
  state and each hidden state of encoder , thus computed compatability scores are passed through softmax function to normalize the scores, these scores are multiplied with each hidden state
  of the encoder to calculate the weighted scores of the encoder hidden states, all these weighted hidden states are added which results in context vector this is passed as one of the inputs
  the Decoder timestep $S_{i}$ along with hidden state of previous decoder timestep, **this lays the foundation for the Attention Mechanism, the attention that we discussed is Bahdanau Attention
  this is also called Additive attention as we are adding all the context vectors to calculate the alignment scores**, this triggered further improvements and `Loung Attention` proposed 
  different ways to calculate alginment scores to calculate the relevance between each hiddent vector of encoder and current decoder state, as part of Loung attention they also managed to avoid
  the alginment model, which reduces the number of parameters to be trained. Below is the reference picture of how Bahdanu Attention works

  ![image](https://github.com/user-attachments/assets/ddd1c4a0-165c-4fc7-a984-d24ea680cb90)

 - **Attention in Transformers**

    The above mentioned attentions **Bahdanau & Luong**  paved way for attention in Transformers, there are few disadvantages with the prior Attention mechanism major one being both of them are 
    sequential in nature, as we process one token after the other this makes training process tedious and time taking, as we see the birth the of Large Language models that are trained on 
    Billions of tokens, this would not have been possible without Self Attention which calculates these Attention scores in parallel which were referred as Alignment scores in Bahdanau & Loung 
    Attentions, to make this parallel processing possible Self Attention follows below steps.

    1. Tokenize Sentence - Breaks the sentence into tokens
    2. Generate Embeddings for the tokens
    3. Pass the Embeddings tokens through  4 different Linear Layers to generate Q,K,V & O matrices, each linear layer has its corresponding weight matrices, $W_{Q}$ , $W_{K}$ $W_{V}$ & $W_{O}$
       these weights are learned through the training process.
       
       - X * $W_{Q}$ = Q - Query Vector 
       - X * $W_{K}$ = K - Key Vector  
       - X * $W_{V}$ = V - Value Vector
       - X * $W_{O}$ = O - Output Vector

       Dimensions:-
       -----------
       - X -> T  * $d_{model}$
       - $W_{Q}$ -> $d_{model}$ * $d_{k}$
       - $W_{K}$ -> $d_{model}$ * $d_{k}$
       - $W_{V}$ -> $d_{model}$ * $d_{k}$
       - $W_{O}$ -> $d_{model}$ * $d_{k}$
         
       - T - Sequence Length
       - $d_{model}$  - Length of Embeddings
       - $d_{k}$ - Output dimensions of $W_{Q}$,$W_{K}$ & $W_{V}$, this can be same as $d_{model}$ as well
   
    5. Calculate the Scaled Dot Product Between Q (Query) & K (Key) vectors to find how each token relates to other token , this is simialr to calculation of alignment scores in earlier Seq-to- 
       Seq RNN models

       Scaled Dot Product Attention: -   $\left( \frac{QK^T}{\sqrt{d_k}} \right)$

       ![image](https://github.com/user-attachments/assets/25c3e110-f985-4d3f-a7ab-78ee290b7dbc)

       
    6. Result of Scaled Dot Product Attention is passed through Softmax to normalize the attention scores
             
       Normalize Attention Scores:-  $\text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)$

    7. Multiply these Attention scores with $W_{V}$ to calculate the weighted attentions
    
    8. Result of the Weighted attentions is thus multiplied by $W_{O}$ output projections.

     Below is the code snippet that explains above steps briefly, though this is not exactly what is being used in Transformer Architecture, as we use Multi Head Attention which we will discuss
     but this is the core of the  Attention calculation

     ```
         import torch
         import torch.nn as nn
         from torch import Tensor
         
            
         class Attention(nn.Module):
           """Single attention head"""
         
           def __init__(self, embedding_dim: int, attention_dim: int):
             super().__init__()
             torch.manual_seed(0)
         
             # Initialising weights
             self.wk = nn.Linear(embedding_dim, attention_dim, bias=False)
             self.wq = nn.Linear(embedding_dim, attention_dim, bias=False) 
             self.wv = nn.Linear(embedding_dim, attention_dim, bias=False) 
         
           def forward(self, embedded: Tensor) -> Tensor:
             # calculating Query, Key and Value
             q = self.wq(embedded)
             k = self.wk(embedded) 
             v = self.wv(embedded) 
             
             # calculating attention scores
             attn_score = q @ torch.transpose(k, -2, -1) / (k.shape[-1] ** 0.5) # [batch_size, num_words, num_words]
         
             # below 2 lines is for masking in decoder block
             upper_triangular  = torch.triu(attn_score, diagonal=1).bool()
             attn_score[upper_triangular] = float("-inf")
         
             # applying softmax
             attn_score_softmax = nn.functional.softmax(attn_score, dim = -1) # [batch_size, num_words, num_words]
         
             # getting weighted values by multiplying softmax of attention score with values
             weighted_values = attn_score_softmax @ v # 
         
             return weighted_values
     ```
      ![image](https://github.com/user-attachments/assets/4e12e1ed-09fe-4719-be95-9e31cdba96c7)

     # Visualizing Self Attention using Llama Model:-
      
      - Download the Llama Model from Hugging Face

        ```
           from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
           import torch
         
           model_name= "meta-llama/Llama-3.2-3B-Instruct"
           tokenizer = AutoTokenizer.from_pretrained(model_name)
           model = AutoModel.from_pretrained(model_name, output_attentions=True)
        ```
      - Llama Model Architecture , Llama model being Decoder only we can see there are only 28 Decoder Layers, no encoder layers
        ```
        LlamaModel(
           (embed_tokens): Embedding(128256, 3072)
           (layers): ModuleList(
             (0-27): 28 x LlamaDecoderLayer(
               (self_attn): LlamaAttention(
                 (q_proj): Linear(in_features=3072, out_features=3072, bias=False)
                 (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
                 (v_proj): Linear(in_features=3072, out_features=1024, bias=False)
                 (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
               )
               (mlp): LlamaMLP(
                 (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
                 (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
                 (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
                 (act_fn): SiLU()
               )
               (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
               (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
             )
           )
           (norm): LlamaRMSNorm((3072,), eps=1e-05)
           (rotary_emb): LlamaRotaryEmbedding()
         )
        ```
      - Tokenize the Input Sentence & Pass it through the Llama Model
        
        ```
          
         import torch
         
         text = "the financial bank is located on river bank"
         inputs = tokenizer(text, return_tensors="pt").to("cuda")
         token_ids = inputs.input_ids[0]
         tokens = tokenizer.convert_ids_to_tokens(token_ids)
         model = model.to("cuda")
         with torch.no_grad():
             inputs = inputs.to("cuda")
             outputs = model(**inputs)

        ```  
     - Get The Attention Matrix from the Outputs, there are 28 Layers , we can see the below dimensions of the `attention_matrix` of length 28 & each layer's attention matrix is of shape               (1,24,9,9) - This is because Llama Model has 24 Heads (This refers to Multi Head attention) and sequence length of tokens that we passed is of length 9 hence the dimension of each head          is 9*9 
       
       ```
          attention_matrix = outputs.attentions
       ```
       <img width="233" alt="image" src="https://github.com/user-attachments/assets/d7219113-ed31-4c54-b36d-366426cec86b" />

       
     - Get Attentions from final layer, calculate the avg attention scores across all heads and plot the heatmap to find relation ship, though the below we can't find stronger contextual
       relation ship between tokens like financial & bank , river & bank we can see them when we go through individual heads of multihead attention, but one thing we can observe in the
       attention score heatmap is all the elements above diagonal are zero. This is because the Decoder part of model has casual attention which prevents each token from attending to future    
       tokens of the sequence, this is important as transformers do the self attention in parallel, where as in RNN the attention always sequentially , hence we don't step on to future
       tokens, in transformers this is not the case as we are processing all the tokens in parallel.

       ```
            import seaborn as sns
            import matplotlib.pyplot as plt
            avg_attn =attention_matrix[27][0].mean(dim=0)
            sns.heatmap(avg_attn.cpu(), cmap="viridis",annot=True,fmt=".2f",xticklabels=tokens,yticklabels=tokens )
            plt.title(f"Attention Matrix (Layer 28)",fontdict={'fontsize':25})
            plt.show()
       ```
       <img width="435" alt="image" src="https://github.com/user-attachments/assets/2e553cb2-22cc-42b6-b42d-fab81d98d13c" />


 
# MULTIHEAD-ATTENTION

# CROSS-ATTENTION

# FLASH-ATTENTION

  Flash Attention is IO Aware & Exact Attention. To understand this, we need to be aware of Vanilla Attention (Self-Attention), which is pivotal for Transformer Architecture. Additionally, having some knowledge of GPU Architecture is beneficial.

**Self-Attention Recap**: In order to calculate Self-Attention, the following steps are performed:

  
   $$
   \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V
   $$

   1. The input embeddings `x` with dimensions (batch_size, sequence_len, n_dim) are passed through three linear layers with weights  $W_q$ ,  $W_k$  &  $W_v$ . As a result, we obtain the matrices **Q**, **K**, and **V**, which have the same dimensions as `x`:

      - \( Q \): Query matrix
      - \( K \): Key matrix
      - \( V \): Value matrix
      - \( d_k \): Dimensionality of the key vectors

 
   2. **Q** - Query Matrix &  **K** -Key Matrix are moved to SM (Streaming Multiprocessor) On-chip Memory for Matrix Multiplication Operation (`GEMM`), Result of this operation is moved to HBM(High Bandwidth Memory) in GPU
   
   3.  We need to apply Masking on the result of Multiplication of **Q** & **$${K^T}$$** to ensure padding tokens get zero probabilities after applying softmax ,
       this result again needs to be moved from HBM to SM On-Chip Memory.
   4.  After applying Masking operation, the same matrix is moved from On-chip Memory to HBM
   5.  Next step would be to apply Softmax operation on the matrix whose size is (batch_size,seq_len,seq_len), to apply softmax the matrix is moved from HBM to On-chip memory.    
   6.  After the Softmax is calculated , result of the same is moved to HBM(High Bandwidth Memory), The size of the Softmax matrix would be of **(batch_size,seq_len,seq_len)**
   7.  Next step is to perform Matrix multiplication between the probabilities(Normalizing the dot product between Q,K) calculated in earlier step using Softmax & the **V** Values matrix whose size is **(batch_size,seq_len,n_dim)**, hence these both matrices need to be moved from HBM to On-Chip memory
   8.  Matrix multiplication is performed between Softmax Values & **V** values matrix to get the final attention score

   From the above steps we can infer that majorly the there are two types of operations one being Matrix Multiplications which is FLOPS(Floating Point Operations), other is data movement       
   between DRAM(HBM) to SRAM (On-Chip Memory), due to massive parallel processing capabilities of GPU Floating point operations are calculated faster , once this is done threads present inside 
   the  **SM** are idle until they get new set of instructions and Data on which these instructions need to be performed , **this makes these operations Memory bound as the time taken to move 
   the data between SRAM (On Chip Memory) & DRAM  is more than the time taken to perform FLOPS (Matrix Multiplicaton in this case)**
   
   
   Flash Attention address this problem by dividing the matrices into multiple blocks , and peforms fusing of kernal operations ( Kernels are functions) , fusing Kernel operations can be 
   considered as chaining different functions on each set of blocks, this fusing of kernel operation reduces the need for storing of intermediate results and memory transfers, also the same 
   calculations are recomputed during backward propagation , instead of storing them and moving them between memory layers, though these two operations increase the number of FLOPS the time 
   taken to calculate the attention matrix is less duration this reduces the I/O operations which is bottleneck in Self 
   Attention.

   
   Flash attention divides the matrix into small tiles and the operations like dot product between Q,${K^T}$ are performed and result of this is passed to another kernel function which 
   calculates mask & passes the output to another function that calculates softmax , furhter this result is passed to another kernel which calculates the dot product between softmax values and 
   V matrix, as these data is passed through multiple kernel functions within SRAM we don't store the intermediate results on HBM.

   
   But here lies the major challenge, inorder to calculate the Softmax we need all the values at once to perfrom sum operation  which is required to calculate(denominator), this is required 
   as we need to divide each element of the dot matrix by sum of all the elments(which is Softmax formula) , as we are dividing the matrix into multiple blocks to perfrom kernel fusion 
   (chaining kernel functions like Dot product, masking and Softmax ) calculating the total sum is not possible ,  hence we need a way to calculate the softmax for these batches accurately,
   fortunately this can be addressed calculatin online softmax, which uses tiling technique which is metioned in NVIDIA researcher [paper](https://arxiv.org/abs/1805.02867), this approach 
   allow us to calculate the softmax for individual blocks and when we are merging them we incrementally calculate the final softmax using the formaula mentioned below until we reach final 
   merging on all the blocks 
   
   <img width="492" alt="image" src="https://github.com/user-attachments/assets/c5f58700-db05-4027-ab1b-a0814ec769cc" />


   $$
   \text{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^{n}\exp(x_j)}
   $$
   
  # Few intresting points to note here is the number of FLOPS (Floating point operations) are more in number than the Self Attention , but the time taken is less compared to Self Attention as we are working on small cunks which makes it faster move the data between HBM and On-Chip memory and On-chip Memory to HBM memory , as we are dividing into multiple chunks this also allows us to increase Sequence Lenght which is Context Length of model, hence we can have more context length for the training model. 

  # [Online Softmax Calculation](https://github.com/varadasantosh/deep-learning-notes/blob/tensorflow/Flash_Attention_Calculations(Online_Softmax).ipynb) 
   
   Reference Links:-

   1. https://horace.io/brrr_intro.html
   2. https://training.continuumlabs.ai/inference/why-is-inference-important/flash-attention-2
   3. https://www.youtube.com/watch?v=IoMSGuiwV3g
   4. https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad#:~:text=So%20basically%2C%20in%20order%20to,statistics%20for%20each%20of%20the
   5. https://www.nvidia.com/en-us/on-demand/session/gtc24-s62546/


   Standard Attention vs Flash Attention from Hugging Face:-
   ------------------
   ![image](https://github.com/user-attachments/assets/8ce6ec2f-2df2-4d5e-b643-598ba3b27097)


# LAYER-NORMALIZATION

# FEEDFORWARD LAYER
