# Transformers
- [Encoder]
   - [Embeddings]
     - [Positional Embeddings](#Positional-Embeddings)
     - [Rotatory Embeddings](#ROTATORY-EMBEDDINGS)
  - Attention
    - [Self Attention](#SELF-ATTENTION)
    - [Multi Head Attention](#MULTIHEAD-ATTENTION)
    - [Cross Attention](#CROSS-ATTENTION)
    - [Flash Attention](#FLASH-ATTENTION)
  - [Layer Normalization](#LAYER-NORMALIZATION)
  - [FeedForward Layer](#FEED-FORWARD-LAYER)

# Encoder

# Positional-Embeddings

# Rotatory-Embeddings

# SELF-ATTENTION

# MULTIHEAD-ATTENTION

# CROSS-ATTENTION

# FLASH-ATTENTION

# LAYER-NORMALIZATION

# FEEDFORWARD LAYER
